{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch_Tutorial_2 : Autograd: Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 노트는 아래의 웹사이트의 코드를 따라가며 해설하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Central to all neural networks in PyTorch is the autograd package\n",
    "- The autograd package provides automatic differentiation for all operations on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tracking all operations on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서의 requires_grad 속성을 True로 설정하면, 대상 텐서에 가해지는 모든 연산을 추적한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# If you set its attribute .requires_grad as True,\n",
    "# it starts to track all operations on it. \n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000020534ACBDC8>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn 속성은 대상 텐서를 만들어낸 연산에 대한 정보를 담고 있다.\n",
    "# 여기서, y는 x에 2를 더하는 연산을 통해 만들어냈음을 알 수 있다.\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) \n",
      " tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3     # 곱하기를 통해 z 텐서를 만들었다.\n",
    "out = z.mean()    # 평균계산을 통해 out 텐서를 만들어냈다.\n",
    "print(z, '\\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[-0.7979,  0.9563],\n",
      "        [ 0.1155,  0.5435]])\n",
      "False \n",
      "\n",
      "a = tensor([[-0.7979,  0.9563],\n",
      "        [ 0.1155,  0.5435]], requires_grad=True)\n",
      "True \n",
      "\n",
      "<SumBackward0 object at 0x0000020534AD8848>\n"
     ]
    }
   ],
   "source": [
    "# PyTorch에서 \"_\"로 끝나는 메소드는 대상을 변화시킨다(In-place).\n",
    "# 따라서, a.requires_grad_()는 텐서 a의 requires_grad 속성값을 바꾼다.\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(\"a =\", a)\n",
    "print(a.requires_grad, '\\n')\n",
    "a.requires_grad_(True)\n",
    "print(\"a =\", a)\n",
    "print(a.requires_grad, '\\n')\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE ) 야코비행렬(Jacobian Marix)이나 vector-Jacobian product에 대한 자세한 설명은 위 사이트 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- backward( )메소드를 통해 대상 텐서와 관련된 모든 gradients가 자동으로 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = 3*(x+2)**2 를 위에서 계산하였다.\n",
    "# out은 scalar이므로, backward()메소드 사용시 argument에 아무값도 넣지 않아도 된다(디폴트로 torch.tensor(1)이 적용됨).\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 다음의 결과를 이해할 것!\n",
    "# 텐서 x의 모든 요소들을 변수로 두고 out을 각각의 변수에 대해 편미분한 후,\n",
    "# 텐서 x의 요소값들을 대입해 편미분값을 구한 것\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward( ) 메소드의 대상텐서가 스칼라가 아니라면?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch의 autograd 패티지는 사실 vector-Jacobian 곱을 구한다.\n",
    "- 이는 야코비행렬(Jacobian Matrix)에 사용자가 backward의 argument로 대입한 vector의 product를 말한다.\n",
    "- 대상 텐서가 스칼라일 경우에 야코비행렬은 스칼라의 gradient가 되고, backward의 argument를 비워놓으면 자동으로 torch.tensor(1)이 대입되므로 최종적으로 vector-Jacobian 곱이 스칼라의 gradient가 된다. 따라서 위에서는 간단히 \"backward가 gradient를 구해준다\"고 한 것이다.\n",
    "- 스칼라가 아닌 경우는 아래의 예시를 참고해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORM :  tensor(1844.2112)\n",
      "tensor([  434.0550,   120.0645, -1788.3779], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(\"NORM : \", y.data.norm())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0960e+02, 4.0960e+03, 4.0960e-01])\n"
     ]
    }
   ],
   "source": [
    "# Now in this case y is no longer a scalar.\n",
    "# torch.autograd could not compute the full Jacobian directly,\n",
    "# but if we just want the vector-Jacobian product,\n",
    "# simply pass the vector to backward as argument:\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 연산추적을 멈추고 싶을 때\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# detach()를 이용할 수도 있다.\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())    # 성분은 모두 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
